[
  {
    "id": "experience",
    "name": "Experience",
    "icon": "üíº",
    "items": [
      {
        "id": "software-engineer",
        "title": "Software Engineer at DICE",
        "description": "Making a dent in the universe through insanely great software.",
        "type": "experience",
        "date": "2021-08-04T00:00:00.000Z",
        "tags": [
          "Release engineering",
          "DICE",
          "C++",
          "C#",
          "CI/CD",
          "Automation"
        ],
        "content": "\n### About Me\n**Nuo Chen, me@nuoc.dev**\n\nI build things that matter. Things that solve real problems for real people. Everything starts with the user experience. Code will grow naturally from it.\n\n### The DICE Journey\n\nFour years of working on 3A games, **Battlefield 2024** and **Battlefield 6**. These aren't just games, they're experiences that touch millions of people.\n\nI joined the Platform team as a Release Engineer. I learned many things. Code base. Tests. CI pipelines. Debugging. The entire lifecycle from code to console.\n\nAnd then I noticed something. Something fundamental.\n\nThe system was broken. Not the code, the process. Engineers were drowning in manual work. They weren't creating. They were fixing. They weren't building the future. They were maintaining the legacy.\n\nHere's also what I noticed: everyone else was building dashboards. Fancy dashboards. Beautiful dashboards. Dashboards to \"help\" engineers monitor the chaos. Dashboards to \"assist\" them in finding problems.\n\nBut that's not the solution. That's just a better way to look at the problem.\n\nI saw this and I thought: *Why not just solve the problem?*\n\nWhy build a dashboard when you can build a system that makes the dashboard unnecessary? Why help engineers find problems when you can make the problems go away?\n\nI wanted to automate the work entirely. Eliminate it. Remove it from their lives. So they can focus on what matters. The fun stuff. The creative stuff. The stuff that made them fall in love with building software in the first place.\n\nThere has to be a better way.\n\nSo I pitched something bold. Something innovative. An automation platform. Not just another dashboard, a complete reimagining of how we work. The leadership saw it. They got it. And it became **Triage Engine**.\n\nI took it from a dream to production. In a massive organization. Tech briefs. ROI Presentations. Workshops. Proof-of-concept. Design. Architecture. Roadmap building. Development. Quality assurance. Rewrites. Stakeholder syncs. Bug fixing. Performance optimizations. I did it all.\n\nIt's running now. In production. Working exactly as designed.\n\n\n### Triage Engine: The Story\n\n**This changes everything.**\n\nI started with a simple truth: Data is power. If you don't own your data, you don't own your AI solution.\n\nTriage Engine isn't just automation. It's a platform. It collects data the way *we* need it. It enables everything that comes after. It learns. It grows. It gets *smarter*.\n\nAt core, it's a .NET background service that does something remarkable: it handles \"Keep Green\" monitoring and investigation automatically. Engineers can focus on what they should be doing, solving problems, not finding them.\n\nIt watches CI pipelines. It detects failures. It gathers context. It builds hypotheses. It assigns ownership. It creates tickets. It verifies resolutions.\n\nIt uses AI. It uses ML. It learns from history. Bayesian probabilistic root cause analysis. Ownership prediction.\n\n**The Timeline:**\n- **PoC:** The vision takes shape\n- **v0.1:** Foundation. Architecture. Data layer\n- **v0.2:** Scale. More users. Real-world refinements\n- **v0.3:** Stability. Now we're pushing further: self-learning, deeper AI\n\n**Under the Hood:**\n\nThis isn't spaghetti code. This is architecture.\n\nComposable, stateless pipelines. Functional programming principles. Distributed services that scale beautifully. Every piece does one thing perfectly. Every piece can be tested independently. Every piece can be replaced without touching the rest. An architecture built for testability.\n\nAnd here's the killer feature: a *language* for defining the process.\n\nWe created a way to describe exactly what matters:\n- What information is critical in each data source\n- How to distinguish real issues from noise\n- How to prioritize what matters most\n- How to define and detect flakiness\n- Where to hunt for evidence to prove hypotheses\n- How to find who owns what problem\n\nThis isn't just software. It's a different way of thinking. It's simpler. It's better. It's the future.\n\n"
      },
      {
        "id": "postgraduate",
        "title": "Postgraduate Student at KTH Royal Institute of Technology",
        "description": "Studied Computer Science with a specialization in Machine Learning at KTH Royal Institute of Technology (Sweden) and completed a master thesis on procedural animation generation using Mixture‚Äëof‚ÄëExperts networks and transfer learning techniques.",
        "type": "experience",
        "date": "2018-08-20T00:00:00.000Z",
        "tags": [
          "KTH",
          "NTU",
          "Computer Science",
          "Machine Learning"
        ],
        "content": "\n### About Me\n**Nuo Chen, me@nuoc.dev**\n\nI pursued my Masters at KTH Royal Institute of Technology in Sweden, focusing on computer science and machine learning.\n\n### Exchange Program at NTU Singapore\n\nStudied fundamentals of machine learning and probability while immersing myself in a diverse cultural environment that broadened my research perspective.\n\n### Research Focus ‚Äì Master Thesis\nMy thesis investigated *Procedural Animation Generation via Mixture‚Äëof‚ÄëExperts (MoE) Networks* that can transfer learned motion patterns between character rigs when only limited target data is available.\n\nKey contributions include:\n- Design of an MoE architecture conditioned on positional and rotational objectives for generating skeletal motions.\n- Integration of several transfer‚Äëlearning strategies (feature‚Äëencoding, feature‚Äëclustering, feature‚Äëselection) to adapt the MoE model across diverse rig domains.\n- Implementation in PyTorch Lightning with a modular training pipeline, automated hyper‚Äëparameter tuning using Ray Tune, and reproducible experiment scripts.\n- Empirical evaluation on multiple datasets showing significant improvements over baseline MLP/LSTM baselines in both quantitative metrics (reconstruction error) and qualitative animation quality.\n\n[Paper](https://kth.diva-portal.org/smash/record.jsf?pid=diva2%3A1635572&dswid=5826)¬†|¬†[Github Repository](https://github.com/Neroro64/Deep-learning-based-rig-agnostic-encoding)"
      }
    ]
  },
  {
    "id": "projects",
    "name": "Projects",
    "icon": "üìÅ",
    "items": [
      {
        "id": "transfer-learning-motion-synthesis",
        "title": "A study of transfer learning on data-driven motion synthesis frameworks",
        "description": "This research explores transfer learning techniques to enhance the scalability and applicability of deep learning-based motion synthesis models for 3D characters in virtual environments.",
        "type": "project",
        "date": "2023-12-05T00:00:00.000Z",
        "tags": [
          "PyTorch",
          "Mixture-of-Experts",
          "MLP",
          "Pytorch-lightning",
          "Ray",
          "Unity"
        ],
        "content": "\n# Deep learning-based rig-agnostic encoding\n[Paper](https://kth.diva-portal.org/smash/record.jsf?pid=diva2%3A1635572&dswid=5826)\n\n[Github Repository](https://github.com/Neroro64/Deep-learning-based-rig-agnostic-encoding)\n\nThis is my master thesis project about studying the possibility of transfer learning for data-driven motion generation frameworks.\nAll necessary code for producing the results described in the thesis are provided here as it is. \n\n**Objective-driven motion generation model architecture**\n\n**Rig-agnostic encoding approaches**\n\n### Abstract\nVarious research has shown the potential and robustness of deep learning-based approaches to synthesise novel motions of 3D characters in virtual environments, such as video games and films.\nThe models are trained with the motion data that is bound to the respective character skeleton (rig).\nIt inflicts a limitation on the scalability and the applicability of the models since they can only learn motions from one particular rig (domain) and produce motions in that domain only.\n\nTransfer learning techniques can be used to overcome this issue and allow the models to better adapt to other domains with limited data.\nThis work presents a study of three transfer learning techniques for the proposed Objective-driven motion generation model (OMG), which is a model for procedurally generating animations conditioned on positional and rotational objectives.\nThree transfer learning approaches for achieving rig-agnostic encoding (RAE) are proposed and experimented with: Feature encoding (FE), Feature clustering (FC) and Feature selection (FS), to improve the learning of the model on new domains with limited data.\n\nAll three approaches demonstrate significant improvement in both the performance and the visual quality of the generated animations, when compared to the vanilla performance.\nThe empirical results indicate that the FE and the FC approaches yield better transferring quality than the FS approach.\nIt is inconclusive which of them performs better, but the FE approach is more computationally efficient, which makes it the more favourable choice for real-time applications.\n \n\n### Implementation and tuning\nThe created motion data are exported from Unity as JSON files, which are parsed and\nextracted to Numpy arrays and stored as bzip¬≠2 compressed binary files.\n\nThe models are implemented in Python using Pytorch and Pytorch¬≠Lightning. \n\nThe implementation of the models are based on [MANN][1], [NSM][2], [LMP¬≠MoE][3], [MVAE][4] and [TRLSTM][5]\nThe implemented models are tested with a small subset of the dataset, to verify\nthe implementation. Ensuring that the reconstruction errors are optimised during\nthe training, and the models are capable of generating correct animations. The\nhyperparameters such as the number of layers, the layer sizes and the learning rates are\ntuned using Ray Tune 4 with ASHA scheduler and a grid search algorithm.\n\n### Contents\n1. [ Jupyter Notebooks ](https://github.com/Neroro64/Deep-learning-based-rig-agnostic-encoding/blob/main/src/notebooks) - contains the notebooks for computing and plotting the results (assuming the models are trained and available).\n2. [ MLP with adversarial net ](https://github.com/Neroro64/Deep-learning-based-rig-agnostic-encoding/blob/main/src/autoencoder/MLP_Adversarial.py) - is the default Autoencoder (3-layer MLP) + an adversarial Conv-LSGAN model for providing the adversarial error of the generated poses.\n3. [ Clustering models ](https://github.com/Neroro64/Deep-learning-based-rig-agnostic-encoding/blob/main/src/clustering_modes) - contains four variants of AE with an extra layer between the encoder and decoder for performing the clustering on the embeddings\n4. [ Experiments ](https://github.com/Neroro64/Deep-learning-based-rig-agnostic-encoding/blob/main/src/experiments) - contains code for training, validating and testing the various models\n5. [ func ](https://github.com/Neroro64/Deep-learning-based-rig-agnostic-encoding/blob/main/src/func) - contains miscellanenous functions for extracting, preparing data\n6. [ motion_generation_models ](https://github.com/Neroro64/Deep-learning-based-rig-agnostic-encoding/blob/main/src/motion_generation_models) - contains the various OMG models and MoGenNet\n\n### References\n[1]: Zhang, He, Starke, Sebastian, Komura, Taku, and Saito, Jun. ‚ÄúMode¬≠adaptive\nneural networks for quadruped motion control‚Äù. In: ACM Transactions on\nGraphics (TOG) 37.4 (2018), pp. 1‚Äì11. ISSN: 0730¬≠0301. DOI: 10.1145/3197517.\n3201366.\n\n[2]: Starke, Sebastian, Zhang, He, Komura, Taku, and Saito, Jun. ‚ÄúNeural state machine\nfor character¬≠scene interactions‚Äù. In: ACM Transactions on Graphics (TOG) 38.6\n(2019), pp. 1‚Äì14. ISSN: 0730¬≠0301. DOI: 10.1145/3355089.3356505.\n\n[3]: Starke, Sebastian, Zhao, Yiwei, Komura, Taku, and Zaman, Kazi. ‚ÄúLocal motion\nphases for learning multi¬≠contact character movements‚Äù. In: ACM Transactions\non Graphics (TOG) 39.4 (2020), 54:1‚Äì54:13. ISSN: 0730¬≠0301. DOI: 10 . 1145 /\n3386569.3392450.\n\n[4]: Ling, Hung Yu, Zinno, Fabio, Cheng, George, and Panne, Michiel Van De.\n‚ÄúCharacter controllers using motion VAEs‚Äù. In: ACM Transactions on Graphics\n(TOG) 39.4 (2020), 40:1‚Äì40:12. ISSN: 0730¬≠0301. DOI: 10 . 1145 / 3386569 .\n3392422.\n\n[5]: Harvey, F√©lix G., Yurick, Mike, Nowrouzezahrai, Derek, and Pal, Christopher.\n‚ÄúRobust motion in¬≠betweening‚Äù. In: ACM Transactions on Graphics (TOG) 39.4\n(2020), 60:1‚Äì60:12. ISSN: 0730¬≠0301. DOI: 10.1145/3386569.3392480."
      },
      {
        "id": "hamils-cube",
        "title": "Hamils Cube",
        "description": "A puzzle-solving mobile game that combines the concept of Rubik's cube and Hamiltonian path, made in Unity.",
        "type": "project",
        "date": "2023-12-05T00:00:00.000Z",
        "tags": [
          "Unity",
          "C#",
          "Blender",
          "Android"
        ],
        "content": "\nA puzzle-solving Android mobile game that combines the concept of Rubik's cube and Hamiltonian path, made in Unity.\n\n### Description\n\nThe game challenges players to solve increasingly complex puzzles by manipulating a 3D grid system where each rotation affects multiple elements simultaneously. The core mechanic revolves around finding the most efficient sequence of moves to reach the target configuration, similar to solving a Rubik's cube but with added complexity from Hamiltonian path constraints that guide optimal solutions.\n\n[Github Repository](https://github.com/Neroro64/Hamil-s-Cube)\n\n### Features\n\n- Puzzle-solving mechanics based on Rubik's cube concepts combined with Hamiltonian path exploration gameplay\n- Mobile-friendly interface\n- Progressive difficulty levels\n\n### Implementation details\n\nDeveloped using Unity with C# scripting, with mesh and animations created in Blender.\n\nThe implementation includes:\n- Custom 3D puzzle grid system with rotation mechanics\n- Touch-based controls optimized for mobile devices\n\n### Tech stack\n\n- **Game Engine**: Unity 2022.3 LTS\n- **Programming Language**: C#\n- **3D Modeling & Animation**: Blender\n- **Version Control**: Git/GitHub\n- **Mobile Platform**: Android/iOS (Unity's cross-platform capabilities)"
      },
      {
        "id": "mirrorception",
        "title": "Mirrorception",
        "description": "Mirrorception is a demo of a platform puzzle game where players seamlessly switch between the real world and its mirrored counterpart by stepping into reflective surfaces.",
        "type": "project",
        "date": "2020-04-13T00:00:00.000Z",
        "tags": [
          "Unity",
          "C#",
          "Blender"
        ],
        "content": "\nMirrorception is a demo of a platform puzzle game, where players must utilize the mirrored version of the world to solve puzzles that would be impossible in the real world. By entering reflective surfaces, players can access alternate dimensions where gravity, object interactions, and spatial relationships behave differently.\n\n[Github repository](https://github.com/Neroro64/Mirrorception)\n\n### Implementation details\n\nThis project was developed in Unity using C#, with most assets created from scratch including 3D models and animations using Blender.\nThe implementation features:\n\n- Custom character controller with mirror-switching mechanics\n- Seamless transition between mirrored and real worlds\n- Interactive environments with reflective surfaces\n- Mirror physics simulation that affects object behavior\n- Puzzle design that requires understanding of mirror properties\n\n### Tech stack\n\n- **Game Engine**: Unity 2022.3 LTS\n- **Programming Language**: C#\n- **3D Modeling & Animation**: Blender"
      }
    ]
  },
  {
    "id": "blog",
    "name": "Blog",
    "icon": "üìù",
    "items": []
  },
  {
    "id": "links",
    "name": "Links",
    "icon": "üîó",
    "items": [
      {
        "id": "gallery",
        "title": "Photo Gallery",
        "description": "A collection of my photography and visual work.",
        "url": "https://gallery.nuoc.dev",
        "icon": "üì∑"
      },
      {
        "id": "notes",
        "title": "Personal documentation",
        "description": "My personal notes and documentation site.",
        "url": "https://notes.nuoc.dev",
        "icon": "üìù"
      },
      {
        "id": "github",
        "title": "GitHub Profile",
        "description": "My GitHub profile with various projects and contributions.",
        "url": "https://github.com/Neroro64",
        "icon": "üêô"
      },
      {
        "id": "linkedin",
        "title": "LinkedIn Profile",
        "description": "Professional networking profile with my work experience.",
        "url": "www.linkedin.com/in/nuo-chen",
        "icon": "üëî"
      }
    ]
  }
]